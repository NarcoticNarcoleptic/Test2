File Path: temp_clone/open-interpreter/tests/test_interpreter.py
import os
from random import randint
import time
import pytest
import interpreter
from interpreter.utils.count_tokens import count_tokens, count_messages_tokens
import time

interpreter.auto_run = True
interpreter.model = "gpt-4"
interpreter.temperature = 0


# this function will run before each test
# we're clearing out the messages Array so we can start fresh and reduce token usage
def setup_function():
    interpreter.reset()
    interpreter.temperature = 0
    interpreter.auto_run = True
    interpreter.model = "gpt-4"
    interpreter.debug_mode = False


# this function will run after each test
# we're introducing some sleep to help avoid timeout issues with the OpenAI API
def teardown_function():
    time.sleep(5)


def test_config_loading():
    # because our test is running from the root directory, we need to do some
    # path manipulation to get the actual path to the config file or our config
    # loader will try to load from the wrong directory and fail
    currentPath = os.path.dirname(os.path.abspath(__file__))
    config_path=os.path.join(currentPath, './config.test.yaml')

    interpreter.extend_config(config_path=config_path)

    # check the settings we configured in our config.test.yaml file
    temperature_ok = interpreter.temperature == 0.25
    model_ok = interpreter.model == "gpt-3.5-turbo"
    debug_mode_ok = interpreter.debug_mode == True

    assert temperature_ok and model_ok and debug_mode_ok

def test_system_message_appending():
    ping_system_message = (
        "Respond to a `ping` with a `pong`. No code. No explanations. Just `pong`."
    )

    ping_request = "ping"
    pong_response = "pong"

    interpreter.system_message += ping_system_message

    messages = interpreter.chat(ping_request)

    assert messages == [
        {"role": "user", "message": ping_request},
        {"role": "assistant", "message": pong_response},
    ]


def test_reset():
    # make sure that interpreter.reset() clears out the messages Array
    assert interpreter.messages == []


def test_token_counter():
    system_tokens = count_tokens(text=interpreter.system_message, model=interpreter.model)
    
    prompt = "How many tokens is this?"

    prompt_tokens = count_tokens(text=prompt, model=interpreter.model)

    messages = [{"role": "system", "message": interpreter.system_message}] + interpreter.messages

    system_token_test = count_messages_tokens(messages=messages, model=interpreter.model)

    system_tokens_ok = system_tokens == system_token_test[0]

    messages.append({"role": "user", "message": prompt})

    prompt_token_test = count_messages_tokens(messages=messages, model=interpreter.model)

    prompt_tokens_ok = system_tokens + prompt_tokens == prompt_token_test[0]

    assert system_tokens_ok and prompt_tokens_ok


def test_hello_world():
    hello_world_response = "Hello, World!"

    hello_world_message = f"Please reply with just the words {hello_world_response} and nothing else. Do not run code. No confirmation just the text."

    messages = interpreter.chat(hello_world_message)

    assert messages == [
        {"role": "user", "message": hello_world_message},
        {"role": "assistant", "message": hello_world_response},
    ]

@pytest.mark.skip(reason="Math is hard")
def test_math():
    # we'll generate random integers between this min and max in our math tests
    min_number = randint(1, 99)
    max_number = randint(1001, 9999)

    n1 = randint(min_number, max_number)
    n2 = randint(min_number, max_number)

    test_result = n1 + n2 * (n1 - n2) / (n2 + n1)

    order_of_operations_message = f"""
    Please perform the calculation `{n1} + {n2} * ({n1} - {n2}) / ({n2} + {n1})` then reply with just the answer, nothing else. No confirmation. No explanation. No words. Do not use commas. Do not show your work. Just return the result of the calculation. Do not introduce the results with a phrase like \"The result of the calculation is...\" or \"The answer is...\"
    
    Round to 2 decimal places.
    """.strip()

    messages = interpreter.chat(order_of_operations_message)

    assert str(round(test_result, 2)) in messages[-1]["message"]


def test_delayed_exec():
    interpreter.chat(
        """Can you write a single block of code and execute it that prints something, then delays 1 second, then prints something else? No talk just code. Thanks!"""
    )

@pytest.mark.skip(reason="This works fine when I run it but fails frequently in Github Actions... will look into it after the hackathon")
def test_nested_loops_and_multiple_newlines():
    interpreter.chat(
        """Can you write a nested for loop in python and shell and run them? Don't forget to properly format your shell script and use semicolons where necessary. Also put 1-3 newlines between each line in the code. Only generate and execute the code. No explanations. Thanks!"""
    )


def test_markdown():
    interpreter.chat(
        """Hi, can you test out a bunch of markdown features? Try writing a fenced code block, a table, headers, everything. DO NOT write the markdown inside a markdown code block, just write it raw."""
    )


File Path: temp_clone/open-interpreter/interpreter/__init__.py
from .core.core import Interpreter
import sys

# This is done so when users `import interpreter`,
# they get an instance of interpreter:

sys.modules["interpreter"] = Interpreter()

# **This is a controversial thing to do,**
# because perhaps modules ought to behave like modules.

# But I think it saves a step, removes friction, and looks good.

#     ____                      ____      __                            __           
#    / __ \____  ___  ____     /  _/___  / /____  _________  ________  / /____  _____
#   / / / / __ \/ _ \/ __ \    / // __ \/ __/ _ \/ ___/ __ \/ ___/ _ \/ __/ _ \/ ___/
#  / /_/ / /_/ /  __/ / / /  _/ // / / / /_/  __/ /  / /_/ / /  /  __/ /_/  __/ /    
#  \____/ .___/\___/_/ /_/  /___/_/ /_/\__/\___/_/  / .___/_/   \___/\__/\___/_/     
#      /_/                                         /_/                               

File Path: temp_clone/open-interpreter/interpreter/code_interpreters/language_map.py
from .languages.python import Python
from .languages.shell import Shell
from .languages.javascript import JavaScript
from .languages.html import HTML
from .languages.applescript import AppleScript
from .languages.r import R
from .languages.powershell import PowerShell


language_map = {
    "python": Python,
    "bash": Shell,
    "shell": Shell,
    "javascript": JavaScript,
    "html": HTML,
    "applescript": AppleScript,
    "r": R,
    "powershell": PowerShell,
}


File Path: temp_clone/open-interpreter/interpreter/code_interpreters/create_code_interpreter.py
from .language_map import language_map

def create_code_interpreter(language):
    # Case in-sensitive
    language = language.lower()

    try:
        CodeInterpreter = language_map[language]
        return CodeInterpreter()
    except KeyError:
        raise ValueError(f"Unknown or unsupported language: {language}")


File Path: temp_clone/open-interpreter/interpreter/code_interpreters/__init__.py


File Path: temp_clone/open-interpreter/interpreter/code_interpreters/subprocess_code_interpreter.py


import subprocess
import threading
import queue
import time
import traceback
from .base_code_interpreter import BaseCodeInterpreter

class SubprocessCodeInterpreter(BaseCodeInterpreter):
    def __init__(self):
        self.start_cmd = ""
        self.process = None
        self.debug_mode = False
        self.output_queue = queue.Queue()
        self.done = threading.Event()

    def detect_active_line(self, line):
        return None
    
    def detect_end_of_execution(self, line):
        return None
    
    def line_postprocessor(self, line):
        return line
    
    def preprocess_code(self, code):
        """
        This needs to insert an end_of_execution marker of some kind,
        which can be detected by detect_end_of_execution.

        Optionally, add active line markers for detect_active_line.
        """
        return code
    
    def terminate(self):
        self.process.terminate()

    def start_process(self):
        if self.process:
            self.terminate()

        self.process = subprocess.Popen(self.start_cmd.split(),
                                        stdin=subprocess.PIPE,
                                        stdout=subprocess.PIPE,
                                        stderr=subprocess.PIPE,
                                        text=True,
                                        bufsize=0,
                                        universal_newlines=True)
        threading.Thread(target=self.handle_stream_output,
                            args=(self.process.stdout, False),
                            daemon=True).start()
        threading.Thread(target=self.handle_stream_output,
                            args=(self.process.stderr, True),
                            daemon=True).start()

    def run(self, code):
        retry_count = 0
        max_retries = 3

        # Setup
        try:
            code = self.preprocess_code(code)
            if not self.process:
                self.start_process()
        except:
            yield {"output": traceback.format_exc()}
            return
            

        while retry_count <= max_retries:
            if self.debug_mode:
                print(f"Running code:\n{code}\n---")

            self.done.clear()

            try:
                self.process.stdin.write(code + "\n")
                self.process.stdin.flush()
                break
            except:
                if retry_count != 0:
                    # For UX, I like to hide this if it happens once. Obviously feels better to not see errors
                    # Most of the time it doesn't matter, but we should figure out why it happens frequently with:
                    # applescript
                    yield {"output": traceback.format_exc()}
                    yield {"output": f"Retrying... ({retry_count}/{max_retries})"}
                    yield {"output": "Restarting process."}

                self.start_process()

                retry_count += 1
                if retry_count > max_retries:
                    yield {"output": "Maximum retries reached. Could not execute code."}
                    return

        while True:
            if not self.output_queue.empty():
                yield self.output_queue.get()
            else:
                time.sleep(0.1)
            try:
                output = self.output_queue.get(timeout=0.3)  # Waits for 0.3 seconds
                yield output
            except queue.Empty:
                if self.done.is_set():
                    # Try to yank 3 more times from it... maybe there's something in there...
                    # (I don't know if this actually helps. Maybe we just need to yank 1 more time)
                    for _ in range(3):
                        if not self.output_queue.empty():
                            yield self.output_queue.get()
                        time.sleep(0.2)
                    break

    def handle_stream_output(self, stream, is_error_stream):
        for line in iter(stream.readline, ''):
            if self.debug_mode:
                print(f"Received output line:\n{line}\n---")

            line = self.line_postprocessor(line)

            if line is None:
                continue # `line = None` is the postprocessor's signal to discard completely

            if self.detect_active_line(line):
                active_line = self.detect_active_line(line)
                self.output_queue.put({"active_line": active_line})
            elif self.detect_end_of_execution(line):
                self.output_queue.put({"active_line": None})
                time.sleep(0.1)
                self.done.set()
            elif is_error_stream and "KeyboardInterrupt" in line:
                self.output_queue.put({"output": "KeyboardInterrupt"})
                time.sleep(0.1)
                self.done.set()
            else:
                self.output_queue.put({"output": line})



File Path: temp_clone/open-interpreter/interpreter/code_interpreters/base_code_interpreter.py


class BaseCodeInterpreter:
    """
    .run is a generator that yields a dict with attributes: active_line, output
    """
    def __init__(self):
        pass

    def run(self, code):
        pass

    def terminate(self):
        pass

File Path: temp_clone/open-interpreter/interpreter/code_interpreters/languages/javascript.py
from ..subprocess_code_interpreter import SubprocessCodeInterpreter
import re

class JavaScript(SubprocessCodeInterpreter):
    file_extension = "js"
    proper_name = "JavaScript"

    def __init__(self):
        super().__init__()
        self.start_cmd = "node -i"
        
    def preprocess_code(self, code):
        return preprocess_javascript(code)
    
    def line_postprocessor(self, line):
        # Node's interactive REPL outputs a billion things
        # So we clean it up:
        if "Welcome to Node.js" in line:
            return None
        if line.strip() in ["undefined", 'Type ".help" for more information.']:
            return None
        # Remove trailing ">"s
        line = re.sub(r'^\s*(>\s*)+', '', line)
        return line

    def detect_active_line(self, line):
        if "## active_line " in line:
            return int(line.split("## active_line ")[1].split(" ##")[0])
        return None

    def detect_end_of_execution(self, line):
        return "## end_of_execution ##" in line
    

def preprocess_javascript(code):
    """
    Add active line markers
    Wrap in a try catch
    Add end of execution marker
    """

    # Split code into lines
    lines = code.split("\n")
    processed_lines = []

    for i, line in enumerate(lines, 1):
        # Add active line print
        processed_lines.append(f'console.log("## active_line {i} ##");')
        processed_lines.append(line)

    # Join lines to form the processed code
    processed_code = "\n".join(processed_lines)

    # Wrap in a try-catch and add end of execution marker
    processed_code = f"""
try {{
{processed_code}
}} catch (e) {{
    console.log(e);
}}
console.log("## end_of_execution ##");
"""

    return processed_code

File Path: temp_clone/open-interpreter/interpreter/code_interpreters/languages/shell.py
import platform
from ..subprocess_code_interpreter import SubprocessCodeInterpreter
import os

class Shell(SubprocessCodeInterpreter):
    file_extension = "sh"
    proper_name = "Shell"

    def __init__(self):
        super().__init__()

        # Determine the start command based on the platform
        if platform.system() == 'Windows':
            self.start_cmd = 'cmd.exe'
        else:
            self.start_cmd = os.environ.get('SHELL', 'bash')

    def preprocess_code(self, code):
        return preprocess_shell(code)
    
    def line_postprocessor(self, line):
        return line

    def detect_active_line(self, line):
        if "## active_line " in line:
            return int(line.split("## active_line ")[1].split(" ##")[0])
        return None

    def detect_end_of_execution(self, line):
        return "## end_of_execution ##" in line
        

def preprocess_shell(code):
    """
    Add active line markers
    Wrap in a try except (trap in shell)
    Add end of execution marker
    """
    
    # Add commands that tell us what the active line is
    code = add_active_line_prints(code)
    
    # Add end command (we'll be listening for this so we know when it ends)
    code += '\necho "## end_of_execution ##"'
    
    return code


def add_active_line_prints(code):
    """
    Add echo statements indicating line numbers to a shell string.
    """
    lines = code.split('\n')
    for index, line in enumerate(lines):
        # Insert the echo command before the actual line
        lines[index] = f'echo "## active_line {index + 1} ##"\n{line}'
    return '\n'.join(lines)

File Path: temp_clone/open-interpreter/interpreter/code_interpreters/languages/python.py
import os
import sys
from ..subprocess_code_interpreter import SubprocessCodeInterpreter
import ast
import re
import shlex

class Python(SubprocessCodeInterpreter):
    file_extension = "py"
    proper_name = "Python"

    def __init__(self):
        super().__init__()
        executable = sys.executable
        if os.name != 'nt':  # not Windows
            executable = shlex.quote(executable)
        self.start_cmd = executable + " -i -q -u"
        
    def preprocess_code(self, code):
        return preprocess_python(code)
    
    def line_postprocessor(self, line):
        if re.match(r'^(\s*>>>\s*|\s*\.\.\.\s*)', line):
            return None
        return line

    def detect_active_line(self, line):
        if "## active_line " in line:
            return int(line.split("## active_line ")[1].split(" ##")[0])
        return None

    def detect_end_of_execution(self, line):
        return "## end_of_execution ##" in line
    

def preprocess_python(code):
    """
    Add active line markers
    Wrap in a try except
    Add end of execution marker
    """

    # Add print commands that tell us what the active line is
    code = add_active_line_prints(code)

    # Wrap in a try except
    code = wrap_in_try_except(code)

    # Remove any whitespace lines, as this will break indented blocks
    # (are we sure about this? test this)
    code_lines = code.split("\n")
    code_lines = [c for c in code_lines if c.strip() != ""]
    code = "\n".join(code_lines)

    # Add end command (we'll be listening for this so we know when it ends)
    code += '\n\nprint("## end_of_execution ##")'

    return code


def add_active_line_prints(code):
    """
    Add print statements indicating line numbers to a python string.
    """
    tree = ast.parse(code)
    transformer = AddLinePrints()
    new_tree = transformer.visit(tree)
    return ast.unparse(new_tree)


class AddLinePrints(ast.NodeTransformer):
    """
    Transformer to insert print statements indicating the line number
    before every executable line in the AST.
    """

    def insert_print_statement(self, line_number):
        """Inserts a print statement for a given line number."""
        return ast.Expr(
            value=ast.Call(
                func=ast.Name(id='print', ctx=ast.Load()),
                args=[ast.Constant(value=f"## active_line {line_number} ##")],
                keywords=[]
            )
        )

    def process_body(self, body):
        """Processes a block of statements, adding print calls."""
        new_body = []

        # In case it's not iterable:
        if not isinstance(body, list):
            body = [body]

        for sub_node in body:
            if hasattr(sub_node, 'lineno'):
                new_body.append(self.insert_print_statement(sub_node.lineno))
            new_body.append(sub_node)

        return new_body

    def visit(self, node):
        """Overridden visit to transform nodes."""
        new_node = super().visit(node)

        # If node has a body, process it
        if hasattr(new_node, 'body'):
            new_node.body = self.process_body(new_node.body)

        # If node has an orelse block (like in for, while, if), process it
        if hasattr(new_node, 'orelse') and new_node.orelse:
            new_node.orelse = self.process_body(new_node.orelse)

        # Special case for Try nodes as they have multiple blocks
        if isinstance(new_node, ast.Try):
            for handler in new_node.handlers:
                handler.body = self.process_body(handler.body)
            if new_node.finalbody:
                new_node.finalbody = self.process_body(new_node.finalbody)

        return new_node
    

def wrap_in_try_except(code):
    # Add import traceback
    code = "import traceback\n" + code

    # Parse the input code into an AST
    parsed_code = ast.parse(code)

    # Wrap the entire code's AST in a single try-except block
    try_except = ast.Try(
        body=parsed_code.body,
        handlers=[
            ast.ExceptHandler(
                type=ast.Name(id="Exception", ctx=ast.Load()),
                name=None,
                body=[
                    ast.Expr(
                        value=ast.Call(
                            func=ast.Attribute(value=ast.Name(id="traceback", ctx=ast.Load()), attr="print_exc", ctx=ast.Load()),
                            args=[],
                            keywords=[]
                        )
                    ),
                ]
            )
        ],
        orelse=[],
        finalbody=[]
    )

    # Assign the try-except block as the new body
    parsed_code.body = [try_except]

    # Convert the modified AST back to source code
    return ast.unparse(parsed_code)


File Path: temp_clone/open-interpreter/interpreter/code_interpreters/languages/html.py
import webbrowser
import tempfile
import os
from ..base_code_interpreter import BaseCodeInterpreter

class HTML(BaseCodeInterpreter):
    file_extension = "html"
    proper_name = "HTML"

    def __init__(self):
        super().__init__()

    def run(self, code):
        # Create a temporary HTML file with the content
        with tempfile.NamedTemporaryFile(delete=False, suffix=".html") as f:
            f.write(code.encode())

        # Open the HTML file with the default web browser
        webbrowser.open('file://' + os.path.realpath(f.name))

        yield {"output": f"Saved to {os.path.realpath(f.name)} and opened with the user's default web browser."}

File Path: temp_clone/open-interpreter/interpreter/code_interpreters/languages/r.py
from ..subprocess_code_interpreter import SubprocessCodeInterpreter
import re

class R(SubprocessCodeInterpreter):
    file_extension = "r"
    proper_name = "R"

    def __init__(self):
        super().__init__()
        self.start_cmd = "R -q --vanilla"  # Start R in quiet and vanilla mode
        
    def preprocess_code(self, code):
        """
        Add active line markers
        Wrap in a tryCatch for better error handling in R
        Add end of execution marker
        """

        lines = code.split("\n")
        processed_lines = []

        for i, line in enumerate(lines, 1):
            # Add active line print
            processed_lines.append(f'cat("## active_line {i} ##\\n");{line}')

        # Join lines to form the processed code
        processed_code = "\n".join(processed_lines)

        # Wrap in a tryCatch for error handling and add end of execution marker
        processed_code = f"""
tryCatch({{
{processed_code}
}}, error=function(e){{
    cat("## execution_error ##\\n", conditionMessage(e), "\\n");
}})
cat("## end_of_execution ##\\n");
"""
        # Count the number of lines of processed_code
        # (R echoes all code back for some reason, but we can skip it if we track this!)
        self.code_line_count = len(processed_code.split("\n")) - 1
        
        return processed_code
    
    def line_postprocessor(self, line):
        # If the line count attribute is set and non-zero, decrement and skip the line
        if hasattr(self, "code_line_count") and self.code_line_count > 0:
            self.code_line_count -= 1
            return None

        if re.match(r'^(\s*>>>\s*|\s*\.\.\.\s*|\s*>\s*|\s*\+\s*|\s*)$', line):
            return None
        if "R version" in line:  # Startup message
            return None
        if line.strip().startswith("[1] \"") and line.endswith("\""):  # For strings, trim quotation marks
            return line[5:-1].strip()
        if line.strip().startswith("[1]"):  # Normal R output prefix for non-string outputs
            return line[4:].strip()

        return line

    def detect_active_line(self, line):
        if "## active_line " in line:
            return int(line.split("## active_line ")[1].split(" ##")[0])
        return None

    def detect_end_of_execution(self, line):
        return "## end_of_execution ##" in line or "## execution_error ##" in line


File Path: temp_clone/open-interpreter/interpreter/code_interpreters/languages/powershell.py
import platform
import os
from ..subprocess_code_interpreter import SubprocessCodeInterpreter

class PowerShell(SubprocessCodeInterpreter):
    file_extension = "ps1"
    proper_name = "PowerShell"

    def __init__(self):
        super().__init__()

        # Determine the start command based on the platform (use "powershell" for Windows)
        if platform.system() == 'Windows':
            self.start_cmd = 'powershell.exe'
            #self.start_cmd = os.environ.get('SHELL', 'powershell.exe')
        else:
            self.start_cmd = os.environ.get('SHELL', 'bash')

    def preprocess_code(self, code):
        return preprocess_powershell(code)

    def line_postprocessor(self, line):
        return line

    def detect_active_line(self, line):
        if "## active_line " in line:
            return int(line.split("## active_line ")[1].split(" ##")[0])
        return None

    def detect_end_of_execution(self, line):
        return "## end_of_execution ##" in line

def preprocess_powershell(code):
    """
    Add active line markers
    Wrap in try-catch block
    Add end of execution marker
    """
    # Add commands that tell us what the active line is
    code = add_active_line_prints(code)

    # Wrap in try-catch block for error handling
    code = wrap_in_try_catch(code)

    # Add end marker (we'll be listening for this to know when it ends)
    code += '\nWrite-Output "## end_of_execution ##"'

    return code

def add_active_line_prints(code):
    """
    Add Write-Output statements indicating line numbers to a PowerShell script.
    """
    lines = code.split('\n')
    for index, line in enumerate(lines):
        # Insert the Write-Output command before the actual line
        lines[index] = f'Write-Output "## active_line {index + 1} ##"\n{line}'
    return '\n'.join(lines)

def wrap_in_try_catch(code):
    """
    Wrap PowerShell code in a try-catch block to catch errors and display them.
    """
    try_catch_code = """
try {
    $ErrorActionPreference = "Stop"
"""
    return try_catch_code + code + "\n} catch {\n    Write-Error $_\n}\n"

File Path: temp_clone/open-interpreter/interpreter/code_interpreters/languages/__init__.py


File Path: temp_clone/open-interpreter/interpreter/code_interpreters/languages/applescript.py
import os
from ..subprocess_code_interpreter import SubprocessCodeInterpreter

class AppleScript(SubprocessCodeInterpreter):
    file_extension = "applescript"
    proper_name = "AppleScript"

    def __init__(self):
        super().__init__()
        self.start_cmd = os.environ.get('SHELL', '/bin/zsh')

    def preprocess_code(self, code):
        """
        Inserts an end_of_execution marker and adds active line indicators.
        """
        # Add active line indicators to the code
        code = self.add_active_line_indicators(code)

        # Escape double quotes
        code = code.replace('"', r'\"')
        
        # Wrap in double quotes
        code = '"' + code + '"'
        
        # Prepend start command for AppleScript
        code = "osascript -e " + code

        # Append end of execution indicator
        code += '; echo "## end_of_execution ##"'
        
        return code

    def add_active_line_indicators(self, code):
        """
        Adds log commands to indicate the active line of execution in the AppleScript.
        """
        modified_lines = []
        lines = code.split('\n')

        for idx, line in enumerate(lines):
            # Add log command to indicate the line number
            if line.strip():  # Only add if line is not empty
                modified_lines.append(f'log "## active_line {idx + 1} ##"')
            modified_lines.append(line)

        return '\n'.join(modified_lines)

    def detect_active_line(self, line):
        """
        Detects active line indicator in the output.
        """
        prefix = "## active_line "
        if prefix in line:
            try:
                return int(line.split(prefix)[1].split()[0])
            except:
                pass
        return None

    def detect_end_of_execution(self, line):
        """
        Detects end of execution marker in the output.
        """
        return "## end_of_execution ##" in line

File Path: temp_clone/open-interpreter/interpreter/cli/cli.py
import argparse
import subprocess
import os
import platform
import pkg_resources
import ooba
import appdirs
from ..utils.get_config import get_config_path
from ..terminal_interface.conversation_navigator import conversation_navigator

arguments = [
    {
        "name": "system_message",
        "nickname": "s",
        "help_text": "prompt / custom instructions for the language model",
        "type": str,
    },
    {"name": "local", "nickname": "l", "help_text": "run the language model locally (experimental)", "type": bool},
    {
        "name": "auto_run",
        "nickname": "y",
        "help_text": "automatically run the interpreter",
        "type": bool,
    },
    {
        "name": "debug_mode",
        "nickname": "d",
        "help_text": "run in debug mode",
        "type": bool,
    },
    {
        "name": "model",
        "nickname": "m",
        "help_text": "model to use for the language model",
        "type": str,
    },
    {
        "name": "temperature",
        "nickname": "t",
        "help_text": "optional temperature setting for the language model",
        "type": float,
    },
    {
        "name": "context_window",
        "nickname": "c",
        "help_text": "optional context window size for the language model",
        "type": int,
    },
    {
        "name": "max_tokens",
        "nickname": "x",
        "help_text": "optional maximum number of tokens for the language model",
        "type": int,
    },
    {
        "name": "max_budget",
        "nickname": "b",
        "help_text": "optionally set the max budget (in USD) for your llm calls",
        "type": float,
    },
    {
        "name": "api_base",
        "nickname": "ab",
        "help_text": "optionally set the API base URL for your llm calls (this will override environment variables)",
        "type": str,
    },
    {
        "name": "api_key",
        "nickname": "ak",
        "help_text": "optionally set the API key for your llm calls (this will override environment variables)",
        "type": str,
    },
    {
        "name": "safe_mode",
        "nickname": "safe",
        "help_text": "optionally enable safety mechanisms like code scanning; valid options are off, ask, and auto",
        "type": str,
        "choices": ["off", "ask", "auto"],
        "default": "off"
    },
    {
        "name": "gguf_quality",
        "nickname": "q",
        "help_text": "(experimental) value from 0-1 which will select the gguf quality/quantization level. lower = smaller, faster, more quantized",
        "type": float,
    },
    {
        "name": "config_file",
        "nickname": "cf",
        "help_text": "optionally set a custom config file to use",
        "type": str,
    },
]


def cli(interpreter):
    parser = argparse.ArgumentParser(description="Open Interpreter")

    # Add arguments
    for arg in arguments:
        if arg["type"] == bool:
            parser.add_argument(
                f'-{arg["nickname"]}',
                f'--{arg["name"]}',
                dest=arg["name"],
                help=arg["help_text"],
                action="store_true",
                default=None,
            )
        else:
            choices = arg["choices"] if "choices" in arg else None
            default = arg["default"] if "default" in arg else None

            parser.add_argument(
                f'-{arg["nickname"]}',
                f'--{arg["name"]}',
                dest=arg["name"],
                help=arg["help_text"],
                type=arg["type"],
                choices=choices,
                default=default,
            )

    # Add special arguments
    parser.add_argument(
        "--config",
        dest="config",
        action="store_true",
        help="open config.yaml file in text editor",
    )
    parser.add_argument(
        "--conversations",
        dest="conversations",
        action="store_true",
        help="list conversations to resume",
    )
    parser.add_argument(
        "-f",
        "--fast",
        dest="fast",
        action="store_true",
        help="(deprecated) runs `interpreter --model gpt-3.5-turbo`",
    )
    parser.add_argument(
        "--version",
        dest="version",
        action="store_true",
        help="get Open Interpreter's version number",
    )
    parser.add_argument(
        '--change_local_device',
        dest='change_local_device',
        action='store_true',
        help="change the device used for local execution (if GPU fails, will use CPU)"
    )

    # TODO: Implement model explorer
    # parser.add_argument('--models', dest='models', action='store_true', help='list avaliable models')

    args = parser.parse_args()

    # This should be pushed into an open_config.py util
    # If --config is used, open the config.yaml file in the Open Interpreter folder of the user's config dir
    if args.config:
        if args.config_file:
            config_file = get_config_path(args.config_file)
        else:
            config_file = get_config_path()

        print(f"Opening `{config_file}`...")

        # Use the default system editor to open the file
        if platform.system() == "Windows":
            os.startfile(
                config_file
            )  # This will open the file with the default application, e.g., Notepad
        else:
            try:
                # Try using xdg-open on non-Windows platforms
                subprocess.call(["xdg-open", config_file])
            except FileNotFoundError:
                # Fallback to using 'open' on macOS if 'xdg-open' is not available
                subprocess.call(["open", config_file])
        return

    # TODO Implement model explorer
    """
    # If --models is used, list models
    if args.models:
        # If they pick a model, set model to that then proceed
        args.model = model_explorer()
    """

    # Set attributes on interpreter
    for attr_name, attr_value in vars(args).items():
        # Ignore things that aren't possible attributes on interpreter
        if attr_value is not None and hasattr(interpreter, attr_name):
            # If the user has provided a config file, load it and extend interpreter's configuration
            if attr_name == "config_file":
                user_config = get_config_path(attr_value)
                interpreter.config_file = user_config
                interpreter.extend_config(config_path=user_config)
            else:
                setattr(interpreter, attr_name, attr_value)

    # if safe_mode and auto_run are enabled, safe_mode disables auto_run
    if interpreter.auto_run and (interpreter.safe_mode == "ask" or interpreter.safe_mode == "auto"):
        setattr(interpreter, "auto_run", False)

    # Default to Mistral if --local is on but --model is unset
    if interpreter.local and args.model is None:
        # This will cause the terminal_interface to walk the user through setting up a local LLM
        interpreter.model = ""

    # If --conversations is used, run conversation_navigator
    if args.conversations:
        conversation_navigator(interpreter)
        return

    if args.version:
        version = pkg_resources.get_distribution("open-interpreter").version
        print(f"Open Interpreter {version}")
        return

    if args.change_local_device:
        print("This will uninstall the experimental local LLM interface (Ooba) in order to reinstall it for a new local device. Proceed? (y/n)")
        if input().lower() == "n":
            return

        print("Please choose your GPU:\n")

        print("A) NVIDIA")
        print("B) AMD (Linux/MacOS only. Requires ROCm SDK 5.4.2/5.4.3 on Linux)")
        print("C) Apple M Series")
        print("D) Intel Arc (IPEX)")
        print("N) None (I want to run models in CPU mode)\n")

        gpu_choice = input("> ").upper()

        while gpu_choice not in 'ABCDN':
            print("Invalid choice. Please try again.")
            gpu_choice = input("> ").upper()

        ooba.install(force_reinstall=True, gpu_choice=gpu_choice, verbose=args.debug_mode)
        return

    # Deprecated --fast
    if args.fast:
        # This will cause the terminal_interface to walk the user through setting up a local LLM
        interpreter.model = "gpt-3.5-turbo"
        print(
            "`interpreter --fast` is deprecated and will be removed in the next version. Please use `interpreter --model gpt-3.5-turbo`"
        )

    interpreter.chat()


File Path: temp_clone/open-interpreter/interpreter/cli/__init__.py


File Path: temp_clone/open-interpreter/interpreter/rag/get_relevant_procedures_string.py
import requests
from ..utils.vector_search import search

def get_relevant_procedures_string(interpreter):

    # Open Procedures is an open-source database of tiny, up-to-date coding tutorials.
    # We can query it semantically and append relevant tutorials/procedures to our system message

    # If download_open_procedures is True and interpreter.procedures is None,
    # We download the bank of procedures:

    if interpreter.procedures is None and interpreter.download_open_procedures and not interpreter.local:
        # Let's get Open Procedures from Github
        url = "https://raw.githubusercontent.com/KillianLucas/open-procedures/main/procedures_db.json"
        response = requests.get(url)
        interpreter._procedures_db = response.json()
        interpreter.procedures = interpreter._procedures_db.keys()

    # Update the procedures database to reflect any changes in interpreter.procedures
    if interpreter._procedures_db.keys() != interpreter.procedures:
        updated_procedures_db = {}
        if interpreter.procedures is not None:
            for key in interpreter.procedures:
                if key in interpreter._procedures_db:
                    updated_procedures_db[key] = interpreter._procedures_db[key]
                else:
                    updated_procedures_db[key] = interpreter.embed_function(key)
        interpreter._procedures_db = updated_procedures_db

    # Assemble the procedures query string. Last two messages
    query_string = ""
    for message in interpreter.messages[-2:]:
        if "content" in message:
            query_string += "\n" + message["content"]
        if "code" in message:
            query_string += "\n" + message["code"]
        if "output" in message:
            query_string += "\n" + message["output"]
    query_string = query_string[-3000:].strip()

    num_results = interpreter.num_procedures

    relevant_procedures = search(query_string, interpreter._procedures_db, interpreter.embed_function, num_results=num_results)

    # This can be done better. Some procedures should just be "sticky"...
    relevant_procedures_string = "[Recommended Procedures]\n" + "\n---\n".join(relevant_procedures) + "\nIn your plan, include steps and, if present, **EXACT CODE SNIPPETS** (especially for deprecation notices, **WRITE THEM INTO YOUR PLAN -- underneath each numbered step** as they will VANISH once you execute your first line of code, so WRITE THEM DOWN NOW if you need them) from the above procedures if they are relevant to the task. Again, include **VERBATIM CODE SNIPPETS** from the procedures above if they are relevent to the task **directly in your plan.**"

    if interpreter.debug_mode:
        print("Generated relevant_procedures_string:", relevant_procedures_string)

    return relevant_procedures_string

File Path: temp_clone/open-interpreter/interpreter/rag/__init__.py


File Path: temp_clone/open-interpreter/interpreter/llm/setup_local_text_llm.py
from ..utils.display_markdown_message import display_markdown_message
import inquirer
import ooba
import html
import copy

def setup_local_text_llm(interpreter):
    """
    Takes an Interpreter (which includes a ton of LLM settings),
    returns a text LLM (an OpenAI-compatible chat LLM with baked-in settings. Only takes `messages`).
    """

    repo_id = interpreter.model.replace("huggingface/", "")

    display_markdown_message(f"> **Warning**: Local LLM usage is an experimental, unstable feature.")

    if repo_id != "TheBloke/Mistral-7B-Instruct-v0.1-GGUF":
        # ^ This means it was prob through the old --local, so we have already displayed this message.
        # Hacky. Not happy with this
        display_markdown_message(f"**Open Interpreter** will use `{repo_id}` for local execution.")

    if "gguf" in repo_id.lower() and interpreter.gguf_quality == None:
        gguf_quality_choices = {
            "Extra Small": 0.0,
            "Small": 0.25,
            "Medium": 0.5,
            "Large": 0.75,
            "Extra Large": 1.0
        }

        questions = [inquirer.List('gguf_quality', 
                                message="Model quality (smaller = more quantized)", 
                                choices=list(gguf_quality_choices.keys()))]
        
        answers = inquirer.prompt(questions)
        interpreter.gguf_quality = gguf_quality_choices[answers['gguf_quality']]

    path = ooba.download(f"https://huggingface.co/{repo_id}")

    ooba_llm = ooba.llm(path, verbose=interpreter.debug_mode)
    print("\nReady.\n")

    def local_text_llm(messages):
        """
        Returns a generator. Makes ooba fully openai compatible
        """

        # I think ooba handles this?
        """
        system_message = messages[0]["content"]
        messages = messages[1:]

        if interpreter.context_window:
            context_window = interpreter.context_window
        else:
            context_window = DEFAULT_CONTEXT_WINDOW

        if interpreter.max_tokens:
            max_tokens = interpreter.max_tokens
        else:
            max_tokens = DEFAULT_MAX_TOKENS
        
        messages = tt.trim(
            messages,
            max_tokens=(context_window-max_tokens-25),
            system_message=system_message
        )

        prompt = messages_to_prompt(messages, interpreter.model)
        """

        # Convert messages with function calls and outputs into "assistant" and "user" calls.
        

        # Align Mistral lol
        if "mistral" in repo_id.lower():
            # just.. let's try a simple system message. this seems to work fine.
            messages[0]["content"] = "You are Open Interpreter. You almost always run code to complete user requests. Outside code, use markdown."
            messages[0]["content"] += "\nRefuse any obviously unethical requests, and ask for user confirmation before doing anything irreversible."

        # Tell it how to run code.
        # THIS MESSAGE IS DUPLICATED IN `setup_text_llm.py`
        # (We should deduplicate it somehow soon. perhaps in the config?)
        
        messages = copy.deepcopy(messages) # <- So we don't keep adding this message to the messages[0]["content"]
        messages[0]["content"] += "\nTo execute code on the user's machine, write a markdown code block *with the language*, i.e:\n\n```python\nprint('Hi!')\n```\nYou will recieve the output ('Hi!'). Use any language."

        if interpreter.debug_mode:
            print("Messages going to ooba:", messages)

        buffer = ''  # Hold potential entity tokens and other characters.

        for token in ooba_llm.chat(messages):
            # Some models like to generate HTML Entities (like &quot;, &amp; &#x27;)
            # instead of symbols in their code when used with Open Interpreter.
            # This is a hack to handle that and convert those entities into actual
            # symbols so that the code can be rendered, parsed, and run accordingly.
            # This could have unintended consequences when generating actual HTML,
            # where you may need actual HTML Entities.

            buffer += token

            # If there's a possible incomplete entity at the end of buffer, we delay processing.
            while ('&' in buffer and ';' in buffer) or (buffer.count('&') == 1 and ';' not in buffer):
                # Find the first complete entity in the buffer.
                start_idx = buffer.find('&')
                end_idx = buffer.find(';', start_idx)

                # If there's no complete entity, break and await more tokens.
                if start_idx == -1 or end_idx == -1:
                    break

                # Yield content before the entity.
                for char in buffer[:start_idx]:
                    yield make_chunk(char)
                
                # Extract the entity, decode it, and yield.
                entity = buffer[start_idx:end_idx + 1]
                yield make_chunk(html.unescape(entity))

                # Remove the processed content from the buffer.
                buffer = buffer[end_idx + 1:]

            # If there's no '&' left in the buffer, yield all of its content.
            if '&' not in buffer:
                for char in buffer:
                    yield make_chunk(char)
                buffer = ''

        # At the end, if there's any content left in the buffer, yield it.
        for char in buffer:
            yield make_chunk(char)
      
    return local_text_llm

def make_chunk(token):
    return {
        "choices": [
            {
                "delta": {
                    "content": token
                }
            }
        ]
    }


File Path: temp_clone/open-interpreter/interpreter/llm/setup_openai_coding_llm.py
import litellm
from ..utils.merge_deltas import merge_deltas
from ..utils.parse_partial_json import parse_partial_json
from ..utils.convert_to_openai_messages import convert_to_openai_messages
from ..utils.display_markdown_message import display_markdown_message
import tokentrim as tt


function_schema = {
  "name": "execute",
  "description":
  "Executes code on the user's machine, **in the users local environment**, and returns the output",
  "parameters": {
    "type": "object",
    "properties": {
      "language": {
        "type": "string",
        "description":
        "The programming language (required parameter to the `execute` function)",
        "enum": ["python", "R", "shell", "applescript", "javascript", "html", "powershell"]
      },
      "code": {
        "type": "string",
        "description": "The code to execute (required)"
      }
    },
    "required": ["language", "code"]
  },
}

def setup_openai_coding_llm(interpreter):
    """
    Takes an Interpreter (which includes a ton of LLM settings),
    returns a OI Coding LLM (a generator that takes OI messages and streams deltas with `message`, `language`, and `code`).
    """

    def coding_llm(messages):
        
        # Convert messages
        messages = convert_to_openai_messages(messages, function_calling=True)

        # Add OpenAI's recommended function message
        messages[0]["content"] += "\n\nOnly use the function you have been provided with."

        # Seperate out the system_message from messages
        # (We expect the first message to always be a system_message)
        system_message = messages[0]["content"]
        messages = messages[1:]

        # Trim messages, preserving the system_message
        try:
            messages = tt.trim(messages=messages, system_message=system_message, model=interpreter.model)
        except:
            if interpreter.context_window:
                messages = tt.trim(messages=messages, system_message=system_message, max_tokens=interpreter.context_window)
            else:
                display_markdown_message("""
                **We were unable to determine the context window of this model.** Defaulting to 3000.
                If your model can handle more, run `interpreter --context_window {token limit}` or `interpreter.context_window = {token limit}`.
                """)
                messages = tt.trim(messages=messages, system_message=system_message, max_tokens=3000)

        if interpreter.debug_mode:
            print("Sending this to the OpenAI LLM:", messages)

        # Create LiteLLM generator
        params = {
            'model': interpreter.model,
            'messages': messages,
            'stream': True,
            'functions': [function_schema]
        }

        # Optional inputs
        if interpreter.api_base:
            params["api_base"] = interpreter.api_base
        if interpreter.api_key:
            params["api_key"] = interpreter.api_key
        if interpreter.max_tokens:
            params["max_tokens"] = interpreter.max_tokens
        if interpreter.temperature is not None:
            params["temperature"] = interpreter.temperature
        else:
            params["temperature"] = 0.0

        # These are set directly on LiteLLM
        if interpreter.max_budget:
            litellm.max_budget = interpreter.max_budget
        if interpreter.debug_mode:
            litellm.set_verbose = True

        # Report what we're sending to LiteLLM
        if interpreter.debug_mode:
            print("Sending this to LiteLLM:", params)

        response = litellm.completion(**params)

        accumulated_deltas = {}
        language = None
        code = ""

        for chunk in response:

            if interpreter.debug_mode:
                print("Chunk from LLM", chunk)

            if ('choices' not in chunk or len(chunk['choices']) == 0):
                # This happens sometimes
                continue

            delta = chunk["choices"][0]["delta"]

            # Accumulate deltas
            accumulated_deltas = merge_deltas(accumulated_deltas, delta)

            if interpreter.debug_mode:
                print("Accumulated deltas", accumulated_deltas)

            if "content" in delta and delta["content"]:
                yield {"message": delta["content"]}

            if ("function_call" in accumulated_deltas 
                and "arguments" in accumulated_deltas["function_call"]):

                if ("name" in accumulated_deltas["function_call"] and accumulated_deltas["function_call"]["name"] == "execute"):
                    arguments = accumulated_deltas["function_call"]["arguments"]
                    arguments = parse_partial_json(arguments)

                    if arguments:
                        if (language is None
                            and "language" in arguments
                            and "code" in arguments # <- This ensures we're *finished* typing language, as opposed to partially done
                            and arguments["language"]):
                            language = arguments["language"]
                            yield {"language": language}
                        
                        if language is not None and "code" in arguments:
                            # Calculate the delta (new characters only)
                            code_delta = arguments["code"][len(code):]
                            # Update the code
                            code = arguments["code"]
                            # Yield the delta
                            if code_delta:
                                yield {"code": code_delta}
                    else:
                        if interpreter.debug_mode:
                            print("Arguments not a dict.")

                # 3.5 REALLY likes to halucinate a function named `python` and you can't really fix that, it seems.
                # We just need to deal with it. 
                elif ("name" in accumulated_deltas["function_call"] and accumulated_deltas["function_call"]["name"] == "python"):
                    if interpreter.debug_mode:
                        print("Got direct python call")
                    if (language is None):
                        language = "python"
                        yield {"language": language}

                    if language is not None:
                        # Pull the code string straight out of the "arguments" string
                        code_delta = accumulated_deltas["function_call"]["arguments"][len(code):]
                        # Update the code
                        code = accumulated_deltas["function_call"]["arguments"]
                        # Yield the delta
                        if code_delta:
                            yield {"code": code_delta}

                else:
                    if interpreter.debug_mode:
                        print("GOT BAD FUNCTION CALL: ", accumulated_deltas["function_call"])

                
    return coding_llm

File Path: temp_clone/open-interpreter/interpreter/llm/convert_to_coding_llm.py


from ..utils.convert_to_openai_messages import convert_to_openai_messages
from .setup_text_llm import setup_text_llm

def convert_to_coding_llm(text_llm, debug_mode=False):
    """
    Takes a text_llm
    returns an OI Coding LLM (a generator that takes OI messages and streams deltas with `message`, 'language', and `code`).
    """

    def coding_llm(messages):
        messages = convert_to_openai_messages(messages, function_calling=False)

        inside_code_block = False
        accumulated_block = ""
        language = None
        
        for chunk in text_llm(messages):

            if debug_mode:
                print("Chunk in coding_llm", chunk)

            if ('choices' not in chunk or len(chunk['choices']) == 0):
                # This happens sometimes
                continue
            
            content = chunk['choices'][0]['delta'].get('content', "")
            
            accumulated_block += content

            if accumulated_block.endswith("`"):
                # We might be writing "```" one token at a time.
                continue
            
            # Did we just enter a code block?
            if "```" in accumulated_block and not inside_code_block:
                inside_code_block = True
                accumulated_block = accumulated_block.split("```")[1]

            # Did we just exit a code block?
            if inside_code_block and "```" in accumulated_block:
                return

            # If we're in a code block,
            if inside_code_block:
                
                # If we don't have a `language`, find it
                if language is None and "\n" in accumulated_block:
                    language = accumulated_block.split("\n")[0]

                    # Default to python if not specified
                    if language == "":
                        language = "python"
                    else:
                        #Removes hallucinations containing spaces or non letters.
                        language = ''.join(char for char in language if char.isalpha())

                    output = {"language": language}

                    # If we recieved more than just the language in this chunk, send that
                    if content.split("\n")[1]:
                        output["code"] = content.split("\n")[1]
                    
                    yield output
                
                # If we do have a `language`, send the output as code
                elif language:
                    yield {"code": content}
            
            # If we're not in a code block, send the output as a message
            if not inside_code_block:
                yield {"message": content}

    return coding_llm

File Path: temp_clone/open-interpreter/interpreter/llm/setup_llm.py


from .setup_text_llm import setup_text_llm
from .convert_to_coding_llm import convert_to_coding_llm
from .setup_openai_coding_llm import setup_openai_coding_llm
import os
import litellm

def setup_llm(interpreter):
    """
    Takes an Interpreter (which includes a ton of LLM settings),
    returns a Coding LLM (a generator that streams deltas with `message` and `code`).
    """

    if (not interpreter.local
        and (interpreter.model in litellm.open_ai_chat_completion_models or interpreter.model.startswith("azure/"))):
        # Function calling LLM
        coding_llm = setup_openai_coding_llm(interpreter)
    else:
        text_llm = setup_text_llm(interpreter)
        coding_llm = convert_to_coding_llm(text_llm, debug_mode=interpreter.debug_mode)

    return coding_llm

File Path: temp_clone/open-interpreter/interpreter/llm/setup_text_llm.py


import litellm

from ..utils.display_markdown_message import display_markdown_message
from .setup_local_text_llm import setup_local_text_llm
import os
import tokentrim as tt
import traceback

def setup_text_llm(interpreter):
    """
    Takes an Interpreter (which includes a ton of LLM settings),
    returns a text LLM (an OpenAI-compatible chat LLM with baked-in settings. Only takes `messages`).
    """

    if interpreter.local:

        # Soon, we should have more options for local setup. For now we only have HuggingFace.
        # So we just do that.

        """

        # Download HF models
        if interpreter.model.startswith("huggingface/"):
            # in the future i think we should just have a model_file attribute.
            # this gets set up in the terminal interface / validate LLM settings.
            # then that's passed into this:
            return setup_local_text_llm(interpreter)
        
        # If we're here, it means the user wants to use
        # an OpenAI compatible endpoint running on localhost

        if interpreter.api_base is None:
            raise Exception('''To use Open Interpreter locally, either provide a huggingface model via `interpreter --model huggingface/{huggingface repo name}`
                            or a localhost URL that exposes an OpenAI compatible endpoint by setting `interpreter --api_base {localhost URL}`.''')
        
        # Tell LiteLLM to treat the endpoint as an OpenAI proxy
        model = "custom_openai/" + interpreter.model

        """

        try:
            # Download and use HF model
            return setup_local_text_llm(interpreter)
        except:
            traceback.print_exc()
            # If it didn't work, apologize and switch to GPT-4

            display_markdown_message(f"""
            > Failed to install `{interpreter.model}`.
            \n\n**We have likely not built the proper `{interpreter.model}` support for your system.**
            \n\n(*Running language models locally is a difficult task!* If you have insight into the best way to implement this across platforms/architectures, please join the `Open Interpreter` community Discord, or the `Oobabooga` community Discord, and consider contributing the development of these projects.)
            """)
            
            raise Exception("Architecture not yet supported for local LLM inference via `Oobabooga`. Please run `interpreter` to connect to a cloud model.")

    # Pass remaining parameters to LiteLLM
    def base_llm(messages):
        """
        Returns a generator
        """

        system_message = messages[0]["content"]

        # Tell it how to run code.
        # THIS MESSAGE IS DUPLICATED IN `setup_local_text_llm.py`
        # (We should deduplicate it somehow soon)
        system_message += "\nTo execute code on the user's machine, write a markdown code block *with the language*, i.e:\n\n```python\nprint('Hi!')\n```\n\nYou will receive the output ('Hi!'). Use any language."

        # TODO swap tt.trim for litellm util
        messages = messages[1:]
        if interpreter.context_window and interpreter.max_tokens:
            trim_to_be_this_many_tokens = interpreter.context_window - interpreter.max_tokens - 25 # arbitrary buffer
            messages = tt.trim(messages, system_message=system_message, max_tokens=trim_to_be_this_many_tokens)
        else:
            try:
                messages = tt.trim(messages, system_message=system_message, model=interpreter.model)
            except:
                display_markdown_message("""
                **We were unable to determine the context window of this model.** Defaulting to 3000.
                If your model can handle more, run `interpreter --context_window {token limit}` or `interpreter.context_window = {token limit}`.
                Also, please set max_tokens: `interpreter --max_tokens {max tokens per response}` or `interpreter.max_tokens = {max tokens per response}`
                """)
                messages = tt.trim(messages, system_message=system_message, max_tokens=3000)

        if interpreter.debug_mode:
            print("Passing messages into LLM:", messages)
    
        # Create LiteLLM generator
        params = {
            'model': interpreter.model,
            'messages': messages,
            'stream': True,
        }

        # Optional inputs
        if interpreter.api_base:
            params["api_base"] = interpreter.api_base
        if interpreter.api_key:
            params["api_key"] = interpreter.api_key
        if interpreter.max_tokens:
            params["max_tokens"] = interpreter.max_tokens
        if interpreter.temperature is not None:
            params["temperature"] = interpreter.temperature
        else:
            params["temperature"] = 0.0

        # These are set directly on LiteLLM
        if interpreter.max_budget:
            litellm.max_budget = interpreter.max_budget
        if interpreter.debug_mode:
            litellm.set_verbose = True

        # Report what we're sending to LiteLLM
        if interpreter.debug_mode:
            print("Sending this to LiteLLM:", params)

        return litellm.completion(**params)

    return base_llm


File Path: temp_clone/open-interpreter/interpreter/llm/__init__.py


File Path: temp_clone/open-interpreter/interpreter/utils/check_for_update.py
import requests
import pkg_resources
from packaging import version

def check_for_update():
    # Fetch the latest version from the PyPI API
    response = requests.get(f'https://pypi.org/pypi/open-interpreter/json')
    latest_version = response.json()['info']['version']

    # Get the current version using pkg_resources
    current_version = pkg_resources.get_distribution("open-interpreter").version

    return version.parse(latest_version) > version.parse(current_version)

File Path: temp_clone/open-interpreter/interpreter/utils/get_conversations.py
import os

from ..utils.local_storage_path import get_storage_path

def get_conversations():
    conversations_dir = get_storage_path("conversations")
    json_files = [f for f in os.listdir(conversations_dir) if f.endswith('.json')]
    return json_files

File Path: temp_clone/open-interpreter/interpreter/utils/local_storage_path.py
import os
import appdirs

# Using appdirs to determine user-specific config path
config_dir = appdirs.user_config_dir("Open Interpreter")

def get_storage_path(subdirectory=None):
    if subdirectory is None:
        return config_dir
    else:
        return os.path.join(config_dir, subdirectory)


File Path: temp_clone/open-interpreter/interpreter/utils/merge_deltas.py
import json
import re

def merge_deltas(original, delta):
    """
    Pushes the delta into the original and returns that.

    Great for reconstructing OpenAI streaming responses -> complete message objects.
    """
    for key, value in delta.items():
        if isinstance(value, dict):
            if key not in original:
                original[key] = value
            else:
                merge_deltas(original[key], value)
        else:
            if key in original:
                original[key] += value
            else:
                original[key] = value
    return original

File Path: temp_clone/open-interpreter/interpreter/utils/count_tokens.py
import tiktoken
from litellm import cost_per_token

def count_tokens(text="", model="gpt-4"):
    """
    Count the number of tokens in a string
    """

    encoder = tiktoken.encoding_for_model(model)

    return len(encoder.encode(text))

def token_cost(tokens=0, model="gpt-4"):
    """
    Calculate the cost of the current number of tokens
    """

    (prompt_cost, _) = cost_per_token(model=model, prompt_tokens=tokens)

    return round(prompt_cost, 6)

def count_messages_tokens(messages=[], model=None):
    """
    Count the number of tokens in a list of messages
    """

    tokens_used = 0

    for message in messages:
        if isinstance(message, str):
            tokens_used += count_tokens(message, model=model)
        elif "message" in message:
            tokens_used += count_tokens(message["message"], model=model)

            if "code" in message:
                tokens_used += count_tokens(message["code"], model=model)

            if "output" in message:
                tokens_used += count_tokens(message["output"], model=model)

    prompt_cost = token_cost(tokens_used, model=model)

    return (tokens_used, prompt_cost)



File Path: temp_clone/open-interpreter/interpreter/utils/embed.py
from chromadb.utils.embedding_functions import DefaultEmbeddingFunction as setup_embed
import os
import numpy as np

# Set up the embedding function
os.environ["TOKENIZERS_PARALLELISM"] = "false" # Otherwise setup_embed displays a warning message
try:
    chroma_embedding_function = setup_embed()
except:
    # This does set up a model that we don't strictly need.
    # If it fails, it's not worth breaking everything.
    pass

def embed_function(query):
    return np.squeeze(chroma_embedding_function([query])).tolist()

File Path: temp_clone/open-interpreter/interpreter/utils/display_markdown_message.py
from rich import print as rich_print
from rich.markdown import Markdown
from rich.rule import Rule

def display_markdown_message(message):
    """
    Display markdown message. Works with multiline strings with lots of indentation.
    Will automatically make single line > tags beautiful.
    """

    for line in message.split("\n"):
        line = line.strip()
        if line == "":
            print("")
        elif line == "---":
            rich_print(Rule(style="white"))
        else:
            rich_print(Markdown(line))

    if "\n" not in message and message.startswith(">"):
        # Aesthetic choice. For these tags, they need a space below them
        print("")

File Path: temp_clone/open-interpreter/interpreter/utils/temporary_file.py
import os
import tempfile


def cleanup_temporary_file(temp_file_name, verbose=False):
    """
    clean up temporary file
    """

    try:
        # clean up temporary file
        os.remove(temp_file_name)

        if verbose:
            print(f"Cleaning up temporary file {temp_file_name}")
            print("---")

    except Exception as e:
        print(f"Could not clean up temporary file.")
        print(e)
        print("")


def create_temporary_file(contents, extension=None, verbose=False):
    """
    create a temporary file with the given contents
    """

    try:
        # Create a temporary file
        with tempfile.NamedTemporaryFile(
            mode="w", delete=False, suffix=f".{extension}" if extension else ""
        ) as f:
            f.write(contents)
            temp_file_name = f.name
            f.close()

        if verbose:
            print(f"Created temporary file {temp_file_name}")
            print("---")

        return temp_file_name

    except Exception as e:
        print(f"Could not create temporary file.")
        print(e)
        print("")


File Path: temp_clone/open-interpreter/interpreter/utils/parse_partial_json.py
import json
import re

def parse_partial_json(s):

    # Attempt to parse the string as-is.
    try:
        return json.loads(s)
    except json.JSONDecodeError:
        pass
  
    # Initialize variables.
    new_s = ""
    stack = []
    is_inside_string = False
    escaped = False

    # Process each character in the string one at a time.
    for char in s:
        if is_inside_string:
            if char == '"' and not escaped:
                is_inside_string = False
            elif char == '\n' and not escaped:
                char = '\\n' # Replace the newline character with the escape sequence.
            elif char == '\\':
                escaped = not escaped
            else:
                escaped = False
        else:
            if char == '"':
                is_inside_string = True
                escaped = False
            elif char == '{':
                stack.append('}')
            elif char == '[':
                stack.append(']')
            elif char == '}' or char == ']':
                if stack and stack[-1] == char:
                    stack.pop()
                else:
                    # Mismatched closing character; the input is malformed.
                    return None
        
        # Append the processed character to the new string.
        new_s += char

    # If we're still inside a string at the end of processing, we need to close the string.
    if is_inside_string:
        new_s += '"'

    # Close any remaining open structures in the reverse order that they were opened.
    for closing_char in reversed(stack):
        new_s += closing_char

    # Attempt to parse the modified string as JSON.
    try:
        return json.loads(new_s)
    except json.JSONDecodeError:
        # If we still can't parse the string as JSON, return None to indicate failure.
        return None


File Path: temp_clone/open-interpreter/interpreter/utils/convert_to_openai_messages.py
import json

def convert_to_openai_messages(messages, function_calling=True):
    new_messages = []

    for message in messages:  
        new_message = {
            "role": message["role"],
            "content": ""
        }

        if "message" in message:
            new_message["content"] = message["message"]

        if "code" in message:
            if function_calling:
                new_message["function_call"] = {
                    "name": "execute",
                    "arguments": json.dumps({
                        "language": message["language"],
                        "code": message["code"]
                    }),
                    # parsed_arguments isn't actually an OpenAI thing, it's an OI thing.
                    # but it's soo useful! we use it to render messages to text_llms
                    "parsed_arguments": {
                        "language": message["language"],
                        "code": message["code"]
                    }
                }
            else:
                new_message["content"] += f"""\n\n```{message["language"]}\n{message["code"]}\n```"""
                new_message["content"] = new_message["content"].strip()

        new_messages.append(new_message)

        if "output" in message:
            if function_calling:
                new_messages.append({
                    "role": "function",
                    "name": "execute",
                    "content": message["output"]
                })
            else:
                new_messages.append({
                    "role": "user",
                    "content": "CODE EXECUTED ON USERS MACHINE. OUTPUT (invisible to the user): " + message["output"]
                })

    return new_messages

File Path: temp_clone/open-interpreter/interpreter/utils/get_user_info_string.py
import getpass
import os
import platform

def get_user_info_string():

    username = getpass.getuser()
    current_working_directory = os.getcwd()
    operating_system = platform.system()
    default_shell = os.environ.get('SHELL')

    return f"[User Info]\nName: {username}\nCWD: {current_working_directory}\nSHELL: {default_shell}\nOS: {operating_system}"

File Path: temp_clone/open-interpreter/interpreter/utils/get_local_models_paths.py
import os

from ..utils.local_storage_path import get_storage_path

def get_local_models_paths():
    models_dir = get_storage_path("models")
    files = [os.path.join(models_dir, f) for f in os.listdir(models_dir)]
    return files

File Path: temp_clone/open-interpreter/interpreter/utils/get_config.py
import os
import yaml
from importlib import resources
import shutil

from .local_storage_path import get_storage_path

config_filename = "config.yaml"

user_config_path = os.path.join(get_storage_path(), config_filename)

def get_config_path(path=user_config_path):
    # check to see if we were given a path that exists
    if not os.path.exists(path):
        # check to see if we were given a filename that exists in the config directory
        if os.path.exists(os.path.join(get_storage_path(), path)):
            path = os.path.join(get_storage_path(), path)
        else:
            # check to see if we were given a filename that exists in the current directory
            if os.path.exists(os.path.join(os.getcwd(), path)):
                path = os.path.join(os.path.curdir, path)
            # if we weren't given a path that exists, we'll create a new file
            else:
                # if the user gave us a path that isn't our default config directory
                # but doesn't already exist, let's create it
                if os.path.dirname(path) and not os.path.exists(os.path.dirname(path)):
                    os.makedirs(os.path.dirname(path), exist_ok=True)
                else:
                    # Ensure the user-specific directory exists
                    os.makedirs(get_storage_path(), exist_ok=True)
                    
                    # otherwise, we'll create the file in our default config directory
                    path = os.path.join(get_storage_path(), path)


                # If user's config doesn't exist, copy the default config from the package
                here = os.path.abspath(os.path.dirname(__file__))
                parent_dir = os.path.dirname(here)
                default_config_path = os.path.join(parent_dir, 'config.yaml')

                # Copying the file using shutil.copy
                new_file = shutil.copy(default_config_path, path)

    return path

def get_config(path=user_config_path):
    path = get_config_path(path)

    with open(path, 'r') as file:
        return yaml.safe_load(file)

File Path: temp_clone/open-interpreter/interpreter/utils/check_for_package.py
import importlib.util
import sys

#borrowed from: https://stackoverflow.com/a/1051266/656011
def check_for_package(package):
  if package in sys.modules:
    return True
  elif (spec := importlib.util.find_spec(package)) is not None:
    try:
      module = importlib.util.module_from_spec(spec)

      sys.modules[package] = module
      spec.loader.exec_module(module)

      return True
    except ImportError:
      return False
  else:
    return False

File Path: temp_clone/open-interpreter/interpreter/utils/__init__.py


File Path: temp_clone/open-interpreter/interpreter/utils/truncate_output.py
def truncate_output(data, max_output_chars=2000):
  needs_truncation = False

  message = f'Output truncated. Showing the last {max_output_chars} characters.\n\n'

  # Remove previous truncation message if it exists
  if data.startswith(message):
    data = data[len(message):]
    needs_truncation = True

  # If data exceeds max length, truncate it and add message
  if len(data) > max_output_chars or needs_truncation:
    data = message + data[-max_output_chars:]

  return data

File Path: temp_clone/open-interpreter/interpreter/utils/vector_search.py
from chromadb.utils.distance_functions import cosine
import numpy as np

def search(query, db, embed_function, num_results=2):
    """
    Finds the most similar value from the embeddings dictionary to the query.

    query is a string
    db is of type [{text: embedding}, {text: embedding}, ...]

    Args:
        query (str): The query to which you want to find a similar value.

    Returns:
        str: The most similar value from the embeddings dictionary.
    """

    # Convert the query to an embedding
    query_embedding = embed_function(query)

    # Calculate the cosine distance between the query embedding and each embedding in the database
    distances = {value: cosine(query_embedding, embedding) for value, embedding in db.items()}

    # Sort the values by their distance to the query, and select the top num_results
    most_similar_values = sorted(distances, key=distances.get)[:num_results]

    # Return the most similar values
    return most_similar_values

File Path: temp_clone/open-interpreter/interpreter/utils/scan_code.py
import os
import subprocess
from yaspin import yaspin
from yaspin.spinners import Spinners

from .temporary_file import create_temporary_file, cleanup_temporary_file
from ..code_interpreters.language_map import language_map


def get_language_file_extension(language_name):
    """
    Get the file extension for a given language
    """
    language = language_map[language_name.lower()]

    if language.file_extension:
        return language.file_extension
    else:
        return language


def get_language_proper_name(language_name):
    """
    Get the proper name for a given language
    """
    language = language_map[language_name.lower()]

    if language.proper_name:
        return language.proper_name
    else:
        return language


def scan_code(code, language, interpreter):
    """
    Scan code with semgrep
    """

    temp_file = create_temporary_file(
        code, get_language_file_extension(language), verbose=interpreter.debug_mode
    )

    temp_path = os.path.dirname(temp_file)
    file_name = os.path.basename(temp_file)

    if interpreter.debug_mode:
        print(f"Scanning {language} code in {file_name}")
        print("---")

    # Run semgrep
    try:
        # HACK: we need to give the subprocess shell access so that the semgrep from our pyproject.toml is available
        # the global namespace might have semgrep from guarddog installed, but guarddog is currenlty
        # pinned to an old semgrep version that has issues with reading the semgrep registry
        # while scanning a single file like the temporary one we generate
        # if guarddog solves [#249](https://github.com/DataDog/guarddog/issues/249) we can change this approach a bit
        with yaspin(text="  Scanning code...").green.right.binary as loading:
            scan = subprocess.run(
                f"cd {temp_path} && semgrep scan --config auto --quiet --error {file_name}",
                shell=True,
            )

        if scan.returncode == 0:
            language_name = get_language_proper_name(language)
            print(
                f"  {'Code Scaner: ' if interpreter.safe_mode == 'auto' else ''}No issues were found in this {language_name} code."
            )
            print("")

        # TODO: it would be great if we could capture any vulnerabilities identified by semgrep
        # and add them to the conversation history

    except Exception as e:
        print(f"Could not scan {language} code.")
        print(e)
        print("")  # <- Aesthetic choice

    cleanup_temporary_file(temp_file, verbose=interpreter.debug_mode)


File Path: temp_clone/open-interpreter/interpreter/core/generate_system_message.py
from ..utils.get_user_info_string import get_user_info_string
import traceback

def generate_system_message(interpreter):
    """
    Dynamically generate a system message.

    Takes an interpreter instance,
    returns a string.

    This is easy to replace!
    Just swap out `interpreter.generate_system_message` with another function.
    """

    #### Start with the static system message

    system_message = interpreter.system_message
    

    #### Add dynamic components, like the user's OS, username, etc

    system_message += "\n" + get_user_info_string()
    try:
        system_message += "\n" + interpreter.get_relevant_procedures_string()
    except:
        if interpreter.debug_mode:
            print(traceback.format_exc())
        # In case some folks can't install the embedding model (I'm not sure if this ever happens)
        pass

    return system_message

File Path: temp_clone/open-interpreter/interpreter/core/core.py
"""
This file defines the Interpreter class.
It's the main file. `import interpreter` will import an instance of this class.
"""
from interpreter.utils import display_markdown_message
from ..cli.cli import cli
from ..utils.get_config import get_config, user_config_path
from ..utils.local_storage_path import get_storage_path
from .respond import respond
from ..llm.setup_llm import setup_llm
from ..terminal_interface.terminal_interface import terminal_interface
from ..terminal_interface.validate_llm_settings import validate_llm_settings
from .generate_system_message import generate_system_message
import os
from datetime import datetime
from ..rag.get_relevant_procedures_string import get_relevant_procedures_string
import json
from ..utils.check_for_update import check_for_update
from ..utils.display_markdown_message import display_markdown_message
from ..utils.embed import embed_function


class Interpreter:
    def cli(self):
        cli(self)

    def __init__(self):
        # State
        self.messages = []
        self._code_interpreters = {}

        self.config_file = user_config_path

        # Settings
        self.local = False
        self.auto_run = False
        self.debug_mode = False
        self.max_output = 2000
        self.safe_mode = "off"

        # Conversation history
        self.conversation_history = True
        self.conversation_filename = None
        self.conversation_history_path = get_storage_path("conversations")

        # LLM settings
        self.model = ""
        self.temperature = None
        self.system_message = ""
        self.context_window = None
        self.max_tokens = None
        self.api_base = None
        self.api_key = None
        self.max_budget = None
        self._llm = None
        self.gguf_quality = None

        # Procedures / RAG
        self.procedures = None
        self._procedures_db = {}
        self.download_open_procedures = True
        self.embed_function = embed_function
        # Number of procedures to add to the system message
        self.num_procedures = 2

        # Load config defaults
        self.extend_config(self.config_file)

        # Check for update
        if not self.local:
            # This should actually be pushed into the utility
            if check_for_update():
                display_markdown_message("> **A new version of Open Interpreter is available.**\n>Please run: `pip install --upgrade open-interpreter`\n\n---")

    def extend_config(self, config_path):
        if self.debug_mode:
            print(f'Extending configuration from `{config_path}`')

        config = get_config(config_path)
        self.__dict__.update(config)

    def chat(self, message=None, display=True, stream=False):
        if stream:
            return self._streaming_chat(message=message, display=display)
        
        # If stream=False, *pull* from the stream.
        for _ in self._streaming_chat(message=message, display=display):
            pass
        
        return self.messages
    
    def _streaming_chat(self, message=None, display=True):

        # If we have a display,
        # we can validate our LLM settings w/ the user first
        if display:
            validate_llm_settings(self)

        # Setup the LLM
        if not self._llm:
            self._llm = setup_llm(self)

        # Sometimes a little more code -> a much better experience!
        # Display mode actually runs interpreter.chat(display=False, stream=True) from within the terminal_interface.
        # wraps the vanilla .chat(display=False) generator in a display.
        # Quite different from the plain generator stuff. So redirect to that
        if display:
            yield from terminal_interface(self, message)
            return
        
        # One-off message
        if message or message == "":
            if message == "":
                message = "No entry from user - please suggest something to enter"
            self.messages.append({"role": "user", "message": message})
            yield from self._respond()

            # Save conversation if we've turned conversation_history on
            if self.conversation_history:

                # If it's the first message, set the conversation name
                if not self.conversation_filename:

                    first_few_words = "_".join(self.messages[0]["message"][:25].split(" ")[:-1])
                    for char in "<>:\"/\\|?*!": # Invalid characters for filenames
                        first_few_words = first_few_words.replace(char, "")

                    date = datetime.now().strftime("%B_%d_%Y_%H-%M-%S")
                    self.conversation_filename = "__".join([first_few_words, date]) + ".json"

                # Check if the directory exists, if not, create it
                if not os.path.exists(self.conversation_history_path):
                    os.makedirs(self.conversation_history_path)
                # Write or overwrite the file
                with open(os.path.join(self.conversation_history_path, self.conversation_filename), 'w') as f:
                    json.dump(self.messages, f)
                
            return
        raise Exception("`interpreter.chat()` requires a display. Set `display=True` or pass a message into `interpreter.chat(message)`.")

    def _respond(self):
        yield from respond(self)
            
    def reset(self):
        for code_interpreter in self._code_interpreters.values():
            code_interpreter.terminate()
        self._code_interpreters = {}

        # Reset the two functions below, in case the user set them
        self.generate_system_message = lambda: generate_system_message(self)
        self.get_relevant_procedures_string = lambda: get_relevant_procedures_string(self)

        self.__init__()


    # These functions are worth exposing to developers
    # I wish we could just dynamically expose all of our functions to devs...
    def generate_system_message(self):
        return generate_system_message(self)
    def get_relevant_procedures_string(self):
        return get_relevant_procedures_string(self)


File Path: temp_clone/open-interpreter/interpreter/core/respond.py
from ..code_interpreters.create_code_interpreter import create_code_interpreter
from ..utils.merge_deltas import merge_deltas
from ..utils.display_markdown_message import display_markdown_message
from ..utils.truncate_output import truncate_output
from ..code_interpreters.language_map import language_map
import traceback
import litellm

def respond(interpreter):
    """
    Yields tokens, but also adds them to interpreter.messages. TBH probably would be good to seperate those two responsibilities someday soon
    Responds until it decides not to run any more code or say anything else.
    """

    while True:

        system_message = interpreter.generate_system_message()

        # Create message object
        system_message = {"role": "system", "message": system_message}

        # Create the version of messages that we'll send to the LLM
        messages_for_llm = interpreter.messages.copy()
        messages_for_llm = [system_message] + messages_for_llm

        # It's best to explicitly tell these LLMs when they don't get an output
        for message in messages_for_llm:
            if "output" in message and message["output"] == "":
                message["output"] = "No output"


        ### RUN THE LLM ###

        # Add a new message from the assistant to interpreter's "messages" attribute
        # (This doesn't go to the LLM. We fill this up w/ the LLM's response)
        interpreter.messages.append({"role": "assistant"})

        # Start putting chunks into the new message
        # + yielding chunks to the user
        try:

            # Track the type of chunk that the coding LLM is emitting
            chunk_type = None

            for chunk in interpreter._llm(messages_for_llm):

                # Add chunk to the last message
                interpreter.messages[-1] = merge_deltas(interpreter.messages[-1], chunk)

                # This is a coding llm
                # It will yield dict with either a message, language, or code (or language AND code)

                # We also want to track which it's sending to we can send useful flags.
                # (otherwise pretty much everyone needs to implement this)
                if "message" in chunk and chunk_type != "message":
                    chunk_type = "message"
                    yield {"start_of_message": True}
                elif "language" in chunk and chunk_type != "code":
                    chunk_type = "code"
                    yield {"start_of_code": True}
                if "code" in chunk and chunk_type != "code":
                    # (This shouldn't happen though  ^ "language" should be emitted first, but sometimes GPT-3.5 forgets this)
                    # (But I'm pretty sure we handle that? If it forgets we emit Python anyway?)
                    chunk_type = "code"
                    yield {"start_of_code": True}
                elif "message" not in chunk and chunk_type == "message":
                    chunk_type = None
                    yield {"end_of_message": True}

                yield chunk

            # We don't trigger the end_of_message or end_of_code flag if we actually end on either
            if chunk_type == "message":
                yield {"end_of_message": True}
            elif chunk_type == "code":
                yield {"end_of_code": True}
            
        except litellm.exceptions.BudgetExceededError:
            display_markdown_message(f"""> Max budget exceeded

                **Session spend:** ${litellm._current_cost}
                **Max budget:** ${interpreter.max_budget}

                Press CTRL-C then run `interpreter --max_budget [higher USD amount]` to proceed.
            """)
            break
        # Provide extra information on how to change API keys, if we encounter that error
        # (Many people writing GitHub issues were struggling with this)
        except Exception as e:
            if 'auth' in str(e).lower() or 'api key' in str(e).lower():
                output = traceback.format_exc()
                raise Exception(f"{output}\n\nThere might be an issue with your API key(s).\n\nTo reset your API key (we'll use OPENAI_API_KEY for this example, but you may need to reset your ANTHROPIC_API_KEY, HUGGINGFACE_API_KEY, etc):\n        Mac/Linux: 'export OPENAI_API_KEY=your-key-here',\n        Windows: 'setx OPENAI_API_KEY your-key-here' then restart terminal.\n\n")
            else:
                raise
        
        
        
        ### RUN CODE (if it's there) ###

        if "code" in interpreter.messages[-1]:
            
            if interpreter.debug_mode:
                print("Running code:", interpreter.messages[-1])

            try:
                # What code do you want to run?
                code = interpreter.messages[-1]["code"]

                # Fix a common error where the LLM thinks it's in a Jupyter notebook
                if interpreter.messages[-1]["language"] == "python" and code.startswith("!"):
                    code = code[1:]
                    interpreter.messages[-1]["code"] = code
                    interpreter.messages[-1]["language"] = "shell"

                # Get a code interpreter to run it
                language = interpreter.messages[-1]["language"]
                if language in language_map:
                    if language not in interpreter._code_interpreters:
                        interpreter._code_interpreters[language] = create_code_interpreter(language)
                    code_interpreter = interpreter._code_interpreters[language]
                else:
                    #This still prints the code but don't allow code to run. Let's Open-Interpreter know through output message
                    error_output = f"Error: Open Interpreter does not currently support {language}."
                    print(error_output)

                    interpreter.messages[-1]["output"] = ""
                    output = "\n" + error_output

                    # Truncate output
                    output = truncate_output(output, interpreter.max_output)
                    interpreter.messages[-1]["output"] = output.strip()
                    break

                # Yield a message, such that the user can stop code execution if they want to
                try:
                    yield {"executing": {"code": code, "language": language}}
                except GeneratorExit:
                    # The user might exit here.
                    # We need to tell python what we (the generator) should do if they exit
                    break

                # Yield each line, also append it to last messages' output
                interpreter.messages[-1]["output"] = ""
                for line in code_interpreter.run(code):
                    yield line
                    if "output" in line:
                        output = interpreter.messages[-1]["output"]
                        output += "\n" + line["output"]

                        # Truncate output
                        output = truncate_output(output, interpreter.max_output)

                        interpreter.messages[-1]["output"] = output.strip()

            except:
                output = traceback.format_exc()
                yield {"output": output.strip()}
                interpreter.messages[-1]["output"] = output.strip()

            yield {"end_of_execution": True}

        else:
            # Doesn't want to run code. We're done
            break

    return

File Path: temp_clone/open-interpreter/interpreter/core/__init__.py


File Path: temp_clone/open-interpreter/interpreter/terminal_interface/magic_commands.py
from ..utils.display_markdown_message import display_markdown_message
from ..utils.count_tokens import count_messages_tokens
import json
import os

def handle_undo(self, arguments):
    # Removes all messages after the most recent user entry (and the entry itself).
    # Therefore user can jump back to the latest point of conversation.
    # Also gives a visual representation of the messages removed.

    if len(self.messages) == 0:
      return
    # Find the index of the last 'role': 'user' entry
    last_user_index = None
    for i, message in enumerate(self.messages):
        if message.get('role') == 'user':
            last_user_index = i

    removed_messages = []

    # Remove all messages after the last 'role': 'user'
    if last_user_index is not None:
        removed_messages = self.messages[last_user_index:]
        self.messages = self.messages[:last_user_index]

    print("") # Aesthetics.

    # Print out a preview of what messages were removed.
    for message in removed_messages:
      if 'content' in message and message['content'] != None:
        display_markdown_message(f"**Removed message:** `\"{message['content'][:30]}...\"`")
      elif 'function_call' in message:
        display_markdown_message(f"**Removed codeblock**") # TODO: Could add preview of code removed here.
    
    print("") # Aesthetics.

def handle_help(self, arguments):
    commands_description = {
      "%debug [true/false]": "Toggle debug mode. Without arguments or with 'true', it enters debug mode. With 'false', it exits debug mode.",
      "%reset": "Resets the current session.",
      "%undo": "Remove previous messages and its response from the message history.",
      "%save_message [path]": "Saves messages to a specified JSON path. If no path is provided, it defaults to 'messages.json'.",
      "%load_message [path]": "Loads messages from a specified JSON path. If no path is provided, it defaults to 'messages.json'.",
      "%tokens [prompt]": "EXPERIMENTAL: Calculate the tokens used by the next request based on the current conversation's messages and estimate the cost of that request; optionally provide a prompt to also calulate the tokens used by that prompt and the total amount of tokens that will be sent with the next request",
      "%help": "Show this help message.",
    }

    base_message = [
      "> **Available Commands:**\n\n"
    ]

    # Add each command and its description to the message
    for cmd, desc in commands_description.items():
      base_message.append(f"- `{cmd}`: {desc}\n")

    additional_info = [
      "\n\nFor further assistance, please join our community Discord or consider contributing to the project's development."
    ]

    # Combine the base message with the additional info
    full_message = base_message + additional_info

    display_markdown_message("".join(full_message))


def handle_debug(self, arguments=None):
    if arguments == "" or arguments == "true":
        display_markdown_message("> Entered debug mode")
        print(self.messages)
        self.debug_mode = True
    elif arguments == "false":
        display_markdown_message("> Exited debug mode")
        self.debug_mode = False
    else:
        display_markdown_message("> Unknown argument to debug command.")

def handle_reset(self, arguments):
    self.reset()
    display_markdown_message("> Reset Done")

def default_handle(self, arguments):
    display_markdown_message("> Unknown command")
    handle_help(self,arguments)

def handle_save_message(self, json_path):
    if json_path == "":
      json_path = "messages.json"
    if not json_path.endswith(".json"):
      json_path += ".json"
    with open(json_path, 'w') as f:
      json.dump(self.messages, f, indent=2)

    display_markdown_message(f"> messages json export to {os.path.abspath(json_path)}")

def handle_load_message(self, json_path):
    if json_path == "":
      json_path = "messages.json"
    if not json_path.endswith(".json"):
      json_path += ".json"
    with open(json_path, 'r') as f:
      self.messages = json.load(f)

    display_markdown_message(f"> messages json loaded from {os.path.abspath(json_path)}")

def handle_count_tokens(self, prompt):
    messages = [{"role": "system", "message": self.system_message}] + self.messages

    outputs = []

    if len(self.messages) == 0:
      (conversation_tokens, conversation_cost) = count_messages_tokens(messages=messages, model=self.model)
    else:
      (conversation_tokens, conversation_cost) = count_messages_tokens(messages=messages, model=self.model)

    outputs.append((f"> Tokens sent with next request as context: {conversation_tokens} (Estimated Cost: ${conversation_cost})"))

    if prompt:
      (prompt_tokens, prompt_cost) = count_messages_tokens(messages=[prompt], model=self.model)
      outputs.append(f"> Tokens used by this prompt: {prompt_tokens} (Estimated Cost: ${prompt_cost})")

      total_tokens = conversation_tokens + prompt_tokens
      total_cost = conversation_cost + prompt_cost

      outputs.append(f"> Total tokens for next request with this prompt: {total_tokens} (Estimated Cost: ${total_cost})")

    outputs.append(f"**Note**: This functionality is currently experimental and may not be accurate. Please report any issues you find to the [Open Interpreter GitHub repository](https://github.com/KillianLucas/open-interpreter).")

    display_markdown_message("\n".join(outputs))


def handle_magic_command(self, user_input):
    # split the command into the command and the arguments, by the first whitespace
    switch = {
      "help": handle_help,
      "debug": handle_debug,
      "reset": handle_reset,
      "save_message": handle_save_message,
      "load_message": handle_load_message,
      "undo": handle_undo,
      "tokens": handle_count_tokens,
    }

    user_input = user_input[1:].strip()  # Capture the part after the `%`
    command = user_input.split(" ")[0]
    arguments = user_input[len(command):].strip()
    action = switch.get(command, default_handle)  # Get the function from the dictionary, or default_handle if not found
    action(self, arguments) # Execute the function


File Path: temp_clone/open-interpreter/interpreter/terminal_interface/terminal_interface.py
"""
The terminal interface is just a view. Just handles the very top layer.
If you were to build a frontend this would be a way to do it
"""

try:
    import readline
except ImportError:
    pass

from .components.code_block import CodeBlock
from .components.message_block import MessageBlock
from .magic_commands import handle_magic_command
from ..utils.display_markdown_message import display_markdown_message
from ..utils.truncate_output import truncate_output
from ..utils.scan_code import scan_code
from ..utils.check_for_package import check_for_package


def terminal_interface(interpreter, message):
    if not interpreter.auto_run:
        interpreter_intro_message = [
            "**Open Interpreter** will require approval before running code."
        ]

        if interpreter.safe_mode == "ask" or interpreter.safe_mode == "auto":
            if not check_for_package("semgrep"):
                interpreter_intro_message.append(f"**Safe Mode**: {interpreter.safe_mode}\n\n>Note: **Safe Mode** requires `semgrep` (`pip install semgrep`)")
        else:
            interpreter_intro_message.append(
                "Use `interpreter -y` to bypass this."
            )

        interpreter_intro_message.append("Press `CTRL-C` to exit.")

        display_markdown_message("\n\n".join(interpreter_intro_message) + "\n")
    
    active_block = None

    if message:
        interactive = False
    else:
        interactive = True

    while True:
        try:
            if interactive:
                message = input("> ").strip()
        except KeyboardInterrupt:
            # Exit gracefully
            break

        if message.startswith("%") and interactive:
            handle_magic_command(interpreter, message)
            continue

        # Many users do this
        if message.strip() == "interpreter --local":
            print("Please press CTRL-C then run `interpreter --local`.")
            continue

        # Track if we've ran a code block.
        # We'll use this to determine if we should render a new code block,
        # In the event we get code -> output -> code again
        ran_code_block = False
        render_cursor = True
            
        try:
            for chunk in interpreter.chat(message, display=False, stream=True):
                if interpreter.debug_mode:
                    print("Chunk in `terminal_interface`:", chunk)
                
                # Message
                if "message" in chunk:
                    if active_block is None:
                        active_block = MessageBlock()
                    if active_block.type != "message":
                        active_block.end()
                        active_block = MessageBlock()
                    active_block.message += chunk["message"]
                    render_cursor = True

                # Code
                if "code" in chunk or "language" in chunk:
                    if active_block is None:
                        active_block = CodeBlock()
                    if active_block.type != "code" or ran_code_block:
                        # If the last block wasn't a code block,
                        # or it was, but we already ran it:
                        active_block.end()
                        active_block = CodeBlock()
                    ran_code_block = False
                    render_cursor = True
                
                if "language" in chunk:
                    active_block.language = chunk["language"]
                if "code" in chunk:
                    active_block.code += chunk["code"]
                if "active_line" in chunk:
                    active_block.active_line = chunk["active_line"]

                # Execution notice
                if "executing" in chunk:
                    if not interpreter.auto_run:
                        # OI is about to execute code. The user wants to approve this

                        # End the active block so you can run input() below it
                        active_block.end()

                        should_scan_code = False

                        if not interpreter.safe_mode == "off":
                            if interpreter.safe_mode == "auto":
                                should_scan_code = True
                            elif interpreter.safe_mode == 'ask':
                                response = input("  Would you like to scan this code? (y/n)\n\n  ")
                                print("")  # <- Aesthetic choice

                                if response.strip().lower() == "y":
                                    should_scan_code = True

                        if should_scan_code:
                            # Get code language and actual code from the chunk
                            # We need to give these to semgrep when we start our scan
                            language = chunk["executing"]["language"]
                            code = chunk["executing"]["code"]

                            scan_code(code, language, interpreter)

                        response = input("  Would you like to run this code? (y/n)\n\n  ")
                        print("")  # <- Aesthetic choice

                        if response.strip().lower() == "y":
                            # Create a new, identical block where the code will actually be run
                            # Conveniently, the chunk includes everything we need to do this:
                            active_block = CodeBlock()
                            active_block.margin_top = False # <- Aesthetic choice
                            active_block.language = chunk["executing"]["language"]
                            active_block.code = chunk["executing"]["code"]
                        else:
                            # User declined to run code.
                            interpreter.messages.append({
                                "role": "user",
                                "message": "I have declined to run this code."
                            })
                            break

                # Output
                if "output" in chunk:
                    ran_code_block = True
                    render_cursor = False
                    active_block.output += "\n" + chunk["output"]
                    active_block.output = active_block.output.strip() # <- Aesthetic choice
                    
                    # Truncate output
                    active_block.output = truncate_output(active_block.output, interpreter.max_output)

                if active_block:
                    active_block.refresh(cursor=render_cursor)

                yield chunk

            # (Sometimes -- like if they CTRL-C quickly -- active_block is still None here)
            if active_block:
                active_block.end()
                active_block = None

            if not interactive:
                # Don't loop
                break

        except KeyboardInterrupt:
            # Exit gracefully
            if active_block:
                active_block.end()
                active_block = None
                
            if interactive:
                # (this cancels LLM, returns to the interactive "> " input)
                continue
            else:
                break

File Path: temp_clone/open-interpreter/interpreter/terminal_interface/validate_llm_settings.py
import os
from ..utils.display_markdown_message import display_markdown_message
import time
import inquirer
import litellm
import getpass

def validate_llm_settings(interpreter):
    """
    Interactivley prompt the user for required LLM settings
    """

    # This runs in a while loop so `continue` lets us start from the top
    # after changing settings (like switching to/from local)
    while True:

        if interpreter.local:
            # Ensure model is downloaded and ready to be set up

            if interpreter.model == "":

                # Interactive prompt to download the best local model we know of

                display_markdown_message("""
                **Open Interpreter** will use `Mistral 7B` for local execution.""")

                if interpreter.gguf_quality == None:
                    interpreter.gguf_quality = 0.35

                """
                models = {
                    '7B': 'TheBloke/CodeLlama-7B-Instruct-GGUF',
                    '13B': 'TheBloke/CodeLlama-13B-Instruct-GGUF',
                    '34B': 'TheBloke/CodeLlama-34B-Instruct-GGUF'
                }

                parameter_choices = list(models.keys())
                questions = [inquirer.List('param', message="Parameter count (smaller is faster, larger is more capable)", choices=parameter_choices)]
                answers = inquirer.prompt(questions)
                chosen_param = answers['param']

                interpreter.model = "huggingface/" + models[chosen_param]
                """

                interpreter.model = "huggingface/TheBloke/Mistral-7B-Instruct-v0.1-GGUF"
                
                break

            else:

                # They have selected a model. Have they downloaded it?
                # Break here because currently, this is handled in llm/setup_local_text_llm.py
                # How should we unify all this?
                break
        
        else:
            # Ensure API keys are set as environment variables

            # OpenAI
            if interpreter.model in litellm.open_ai_chat_completion_models:
                if not os.environ.get("OPENAI_API_KEY") and not interpreter.api_key:
                    
                    display_welcome_message_once()

                    display_markdown_message("""---
                    > OpenAI API key not found

                    To use `GPT-4` (recommended) please provide an OpenAI API key.

                    To use `Mistral-7B` (free but less capable) press `enter`.
                    
                    ---
                    """)

                    response = getpass.getpass("OpenAI API key: ")
                    print(f"OpenAI API key: {response[:4]}...{response[-4:]}")

                    if response == "":
                        # User pressed `enter`, requesting Mistral-7B
                        display_markdown_message("""> Switching to `Mistral-7B`...
                        
                        **Tip:** Run `interpreter --local` to automatically use `Mistral-7B`.
                        
                        ---""")
                        time.sleep(1.5)
                        interpreter.local = True
                        interpreter.model = ""
                        continue
                    
                    display_markdown_message("""

                    **Tip:** To save this key for later, run `export OPENAI_API_KEY=your_api_key` on Mac/Linux or `setx OPENAI_API_KEY your_api_key` on Windows.
                    
                    ---""")

                    interpreter.api_key = response
                    time.sleep(2)
                    break

            # This is a model we don't have checks for yet.
            break

    # If we're here, we passed all the checks.

    # Auto-run is for fast, light useage -- no messages.
    # If mistral, we've already displayed a message.
    if not interpreter.auto_run and "mistral" not in interpreter.model.lower():
        display_markdown_message(f"> Model set to `{interpreter.model.upper()}`")
    return


def display_welcome_message_once():
    """
    Displays a welcome message only on its first call.
    
    (Uses an internal attribute `_displayed` to track its state.)
    """
    if not hasattr(display_welcome_message_once, "_displayed"):

        display_markdown_message("""
        

        Welcome to **Open Interpreter**.
        """)
        time.sleep(1.5)

        display_welcome_message_once._displayed = True

File Path: temp_clone/open-interpreter/interpreter/terminal_interface/conversation_navigator.py
"""
This file handles conversations.
"""

import inquirer
import subprocess
import platform
import os
import json
from .render_past_conversation import render_past_conversation
from ..utils.display_markdown_message import display_markdown_message
from ..utils.local_storage_path import get_storage_path

def conversation_navigator(interpreter):

    conversations_dir = get_storage_path("conversations")

    display_markdown_message(f"""> Conversations are stored in "`{conversations_dir}`".
    
    Select a conversation to resume.
    """)

    # Check if conversations directory exists
    if not os.path.exists(conversations_dir):
        print(f"No conversations found in {conversations_dir}")
        return None

    # Get list of all JSON files in the directory
    json_files = [f for f in os.listdir(conversations_dir) if f.endswith('.json')]

    # Make a dict that maps reformatted "First few words... (September 23rd)" -> "First_few_words__September_23rd.json" (original file name)
    readable_names_and_filenames = {}
    for filename in json_files:
        name = filename.replace(".json", "").replace(".JSON", "").replace("__", "... (").replace("_", " ") + ")"
        readable_names_and_filenames[name] = filename

    # Add the option to open the folder. This doesn't map to a filename, we'll catch it
    readable_names_and_filenames["> Open folder"] = None

    # Use inquirer to let the user select a file
    questions = [
        inquirer.List('name',
                      message="",
                      choices=readable_names_and_filenames.keys(),
                      ),
    ]
    answers = inquirer.prompt(questions)

    # If the user selected to open the folder, do so and return
    if answers['name'] == "> Open folder":
        open_folder(conversations_dir)
        return

    selected_filename = readable_names_and_filenames[answers['name']]

    # Open the selected file and load the JSON data
    with open(os.path.join(conversations_dir, selected_filename), 'r') as f:
        messages = json.load(f)

    # Pass the data into render_past_conversation
    render_past_conversation(messages)

    # Set the interpreter's settings to the loaded messages
    interpreter.messages = messages
    interpreter.conversation_filename = selected_filename

    # Start the chat
    interpreter.chat()

def open_folder(path):
    if platform.system() == "Windows":
        os.startfile(path)
    elif platform.system() == "Darwin":
        subprocess.run(["open", path])
    else:
        # Assuming it's Linux
        subprocess.run(["xdg-open", path])

File Path: temp_clone/open-interpreter/interpreter/terminal_interface/__init__.py


File Path: temp_clone/open-interpreter/interpreter/terminal_interface/render_past_conversation.py
from .components.code_block import CodeBlock
from .components.message_block import MessageBlock
from .magic_commands import handle_magic_command
from ..utils.display_markdown_message import display_markdown_message

def render_past_conversation(messages):
    # This is a clone of the terminal interface.
    # So we should probably find a way to deduplicate...

    active_block = None
    render_cursor = False
    ran_code_block = False

    for chunk in messages:
        # Only addition to the terminal interface:
        if chunk["role"] == "user":
            if active_block:
                active_block.end()
                active_block = None
            print(">", chunk["message"])
            continue

        # Message
        if "message" in chunk:
            if active_block is None:
                active_block = MessageBlock()
            if active_block.type != "message":
                active_block.end()
                active_block = MessageBlock()
            active_block.message += chunk["message"]

        # Code
        if "code" in chunk or "language" in chunk:
            if active_block is None:
                active_block = CodeBlock()
            if active_block.type != "code" or ran_code_block:
                # If the last block wasn't a code block,
                # or it was, but we already ran it:
                active_block.end()
                active_block = CodeBlock()
            ran_code_block = False
            render_cursor = True
        
        if "language" in chunk:
            active_block.language = chunk["language"]
        if "code" in chunk:
            active_block.code += chunk["code"]
        if "active_line" in chunk:
            active_block.active_line = chunk["active_line"]

        # Output
        if "output" in chunk:
            ran_code_block = True
            render_cursor = False
            active_block.output += "\n" + chunk["output"]
            active_block.output = active_block.output.strip() # <- Aesthetic choice

        if active_block:
            active_block.refresh(cursor=render_cursor)

    # (Sometimes -- like if they CTRL-C quickly -- active_block is still None here)
    if active_block:
        active_block.end()
        active_block = None

File Path: temp_clone/open-interpreter/interpreter/terminal_interface/components/base_block.py
from rich.live import Live
from rich.console import Console

class BaseBlock:
    """
    a visual "block" on the terminal.
    """
    def __init__(self):
        self.live = Live(auto_refresh=False, console=Console(), vertical_overflow="visible")
        self.live.start()

    def update_from_message(self, message):
        raise NotImplementedError("Subclasses must implement this method")

    def end(self):
        self.refresh(cursor=False)
        self.live.stop()

    def refresh(self, cursor=True):
        raise NotImplementedError("Subclasses must implement this method")

File Path: temp_clone/open-interpreter/interpreter/terminal_interface/components/code_block.py
from rich.panel import Panel
from rich.box import MINIMAL
from rich.syntax import Syntax
from rich.table import Table
from rich.console import Group
from .base_block import BaseBlock

class CodeBlock(BaseBlock):
  """
  Code Blocks display code and outputs in different languages. You can also set the active_line!
  """

  def __init__(self):
    super().__init__()

    self.type = "code"

    # Define these for IDE auto-completion
    self.language = ""
    self.output = ""
    self.code = ""
    self.active_line = None
    self.margin_top = True

  def refresh(self, cursor=True):
    # Get code, return if there is none
    code = self.code
    if not code:
      return
    
    # Create a table for the code
    code_table = Table(show_header=False,
                       show_footer=False,
                       box=None,
                       padding=0,
                       expand=True)
    code_table.add_column()

    # Add cursor    
    if cursor:
      code += ""

    # Add each line of code to the table
    code_lines = code.strip().split('\n')
    for i, line in enumerate(code_lines, start=1):
      if i == self.active_line:
        # This is the active line, print it with a white background
        syntax = Syntax(line, self.language, theme="bw", line_numbers=False, word_wrap=True)
        code_table.add_row(syntax, style="black on white")
      else:
        # This is not the active line, print it normally
        syntax = Syntax(line, self.language, theme="monokai", line_numbers=False, word_wrap=True)
        code_table.add_row(syntax)

    # Create a panel for the code
    code_panel = Panel(code_table, box=MINIMAL, style="on #272722")

    # Create a panel for the output (if there is any)
    if self.output == "" or self.output == "None":
      output_panel = ""
    else:
      output_panel = Panel(self.output,
                           box=MINIMAL,
                           style="#FFFFFF on #3b3b37")

    # Create a group with the code table and output panel
    group_items = [code_panel, output_panel]
    if self.margin_top:
        # This adds some space at the top. Just looks good!
        group_items = [""] + group_items
    group = Group(*group_items)

    # Update the live display
    self.live.update(group)
    self.live.refresh()




File Path: temp_clone/open-interpreter/interpreter/terminal_interface/components/message_block.py
from rich.panel import Panel
from rich.markdown import Markdown
from rich.box import MINIMAL
import re
from .base_block import BaseBlock

class MessageBlock(BaseBlock):

  def __init__(self):
    super().__init__()

    self.type = "message"
    self.message = ""
    self.has_run = False

  def refresh(self, cursor=True):
    # De-stylize any code blocks in markdown,
    # to differentiate from our Code Blocks
    content = textify_markdown_code_blocks(self.message)
    
    if cursor:
      content += ""
      
    markdown = Markdown(content.strip())
    panel = Panel(markdown, box=MINIMAL)
    self.live.update(panel)
    self.live.refresh()


def textify_markdown_code_blocks(text):
  """
  To distinguish CodeBlocks from markdown code, we simply turn all markdown code
  (like '```python...') into text code blocks ('```text') which makes the code black and white.
  """
  replacement = "```text"
  lines = text.split('\n')
  inside_code_block = False

  for i in range(len(lines)):
    # If the line matches ``` followed by optional language specifier
    if re.match(r'^```(\w*)$', lines[i].strip()):
      inside_code_block = not inside_code_block

      # If we just entered a code block, replace the marker
      if inside_code_block:
        lines[i] = replacement

  return '\n'.join(lines)


